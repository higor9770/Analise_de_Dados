# -*- coding: utf-8 -*-
"""Spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HUUhnBhT1Twd23w2bJAucqXNiiQgCQ9V
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q !wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar xf spark-3.4.1-bin-hadoop3.tgz
!pip install -q findspark

!pip install pyspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

import findspark
findspark.init()

# Iniciar uma sessão local e importar dados
from pyspark.sql import SparkSession
sc = SparkSession.builder.master('local[*]').getOrCreate()

# carregar dados do meu computador
dados_spark = sc.read.csv("/content/gasto_cliente.csv", header=True)

# Verificando o tipo de objeto criado
type(dados_spark)

# Verificando o dataset
dados_spark.show(5)

# Verificando o schema() deste spark dataframe
dados_spark.printSchema()

#Retornar o número de linhas
dados_spark.count()

#Selecionar colunas
dados_spark.select("ID_Usuario","Genero","Idade","Ocupacao").show(5)

#Principais estatísticas
dados_spark.describe().show(5)

#Ocorrências em uma variável
dados_spark.select("Cidade_Categoria").distinct().show()

#Ocorrências em uma variável
dados_spark.select("Estado_Civil").distinct().show()

#importar sql: groupBy
from pyspark.sql import functions as F

dados_spark.groupBy("Cidade_Categoria").agg(F.sum("Valor")).show()

item_popular = dados_spark.groupBy('ID_Produto').agg(F.count('ID_Produto')).orderBy(F.count('ID_Produto').desc()).show(10)

idade_popular = dados_spark.groupBy('Idade').agg(F.sum('Valor')).orderBy(F.sum('Valor').desc()).show()

gasto_cliente = dados_spark.groupBy('ID_Usuario').agg(F.count('ID_Usuario'),F.sum('Valor')).orderBy(F.sum('Valor').desc()).show(10)

#Dados faltantes
dados_spark.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in dados_spark.columns]).show()

#substituir valores ausentes
dados_spark_sem_na = dados_spark.fillna({'Categoria_Produto_2':'Sem Resposta', 'Categoria_Produto_3':'Sem Resposta'})

dados_spark_sem_na.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in dados_spark_sem_na.columns]).show()

# Verificando o novo dataset
dados_spark_sem_na.show(5)

#Excluir colunas da base de dados
dados_spark = dados_spark.drop('Categoria_Produto_2','Categoria_Produto_3')
dados_spark.show(5)

# Salvar resultado
dados_spark_sem_na.write.csv("/content/drive/MyDrive/data")

# Spark para Pandas
dados_spark_sem_na_pd = dados_spark_sem_na.toPandas()

# Salvar resultado
dados_spark_sem_na_pd.to_csv("/content/drive/MyDrive/data.csv")

import matplotlib.pyplot as plt

valor = dados_spark_sem_na_pd['Valor']
plt.hist(valor)
plt.show()